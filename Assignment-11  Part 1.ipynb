{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <center> Assignment - 11 Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. What are the three stages to build the hypotheses or model in machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer </b>: Below are the 3 stages to build the hypotheses in Machine Learning\n",
    "\n",
    "a) <b>Model Building:</b> It's also referred to as writing the algorithm which will perform over a set of outputs to give the desired output\n",
    "\n",
    "b)<b> Model Testing:</b> The model has to be trained over a training dataset and specific parameters are chosen. The model is then applied to the dataset over those parameters.\n",
    "\n",
    "c) <b> Applying the model: </b>Apply the dataset over the test dataset to evaluate the performance of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.   What is the standard approach to supervised learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Answer:</b> Machine learning technique for building predictive models from known input and response data. Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values.\n",
    "The standard approach to supervised learning is to split the set of example into the training set and the test. Model once trained on trained data can be used to verify/predict the results on test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is Training set and Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Answer:</b>  Training Set In machine learning, a training set is a dataset used to train a model. In training the model, specific features are picked out from the training set. These features are then incorporated into the model. Thereby, if the training set is labeled correctly, the model should be able to learn something from these features.\n",
    "\n",
    "Test Set The test set is a dataset used to measure how well the model performs at making predictions on that test set. If the prediction scores for the test set are unreasonable, we’ll need to make some adjustments to our model and try again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Answer:</b>\n",
    "\n",
    "<b> Ensemble method:</b> Several models are combined to build a predictive model to give more accurate output.\n",
    "\n",
    "<b> Bagging method:</b> Several estimators are built independently on subsets of the data and their predictions are averaged. Typically the combined estimator is usually better than any of the single base estimator.\n",
    "\n",
    "Bagging can reduce variance with little to no effect on bias.\n",
    "\n",
    "ex: Random Forests\n",
    "\n",
    "<b> Boosting method:</b> It's an approach which combines weak learning rule to a strong learning rule. It combines several weak learning rules to form a single strong learner iteratively.\n",
    "\n",
    "Boosting can reduce bias without incurring higher variance.\n",
    "\n",
    "ex: Gradient Boosted Trees, AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5. How can you avoid overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Answer:</b>\n",
    "\n",
    "\n",
    "\n",
    "Detecting overfitting is useful, but it doesn’t solve the problem.\n",
    "\n",
    "Here are a few of the most popular solutions for overfitting:\n",
    "\n",
    "<b>Cross-validation</b>\n",
    "\n",
    "Cross-validation is a powerful preventative measure against overfitting.\n",
    "\n",
    "The idea is clever: Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n",
    "\n",
    "In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the “holdout fold”).\n",
    "\n",
    "\n",
    "<img src=\"https://elitedatascience.com/wp-content/uploads/2017/06/Cross-Validation-Diagram.jpg\">\n",
    "\n",
    "$$K-Fold Cross-Validation$$\n",
    "\n",
    "Cross-validation allows you to tune hyperparameters with only your original training set. This allows you to keep your test set as a truly unseen dataset for selecting your final model.\n",
    "\n",
    "We have another article with a more detailed breakdown of cross-validation.\n",
    "\n",
    "<b>Train with more data</b>\n",
    "\n",
    "It won’t work every time, but training with more data can help algorithms detect the signal better. In the earlier example of modeling height vs. age in children, it’s clear how sampling more schools will help your model.\n",
    "\n",
    "Of course, that’s not always the case. If we just add more noisy data, this technique won’t help. That’s why you should always ensure your data is clean and relevant.\n",
    "\n",
    "<b>Remove features</b>\n",
    "\n",
    "Some algorithms have built-in feature selection.\n",
    "\n",
    "For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n",
    "\n",
    "An interesting way to do so is to tell a story about how each feature fits into the model. This is like the data scientist's spin on software engineer’s rubber duck debugging technique, where they debug their code by explaining it, line-by-line, to a rubber duck.\n",
    "\n",
    "If anything doesn't make sense, or if it’s hard to justify certain features, this is a good way to identify them. In addition, there are several feature selection heuristics you can use for a good starting point.\n",
    "\n",
    "<b>Early stopping</b>\n",
    "\n",
    "\n",
    "When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.\n",
    "\n",
    "Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n",
    "\n",
    "Early stopping refers stopping the training process before the learner passes that point.\n",
    "\n",
    "Early stopping graphic\n",
    "<img src=\"https://elitedatascience.com/wp-content/uploads/2017/09/early-stopping-graphic.jpg\">\n",
    "Today, this technique is mostly used in deep learning while other techniques (e.g. regularization) are preferred for classical machine learning.\n",
    "\n",
    "<b>Regularization</b>\n",
    "\n",
    "Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.\n",
    "\n",
    "The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
    "\n",
    "Oftentimes, the regularization method is a hyperparameter as well, which means it can be tuned through cross-validation.\n",
    "\n",
    "We have a more detailed discussion here on algorithms and regularization methods.\n",
    "\n",
    "<b>Ensembling</b>\n",
    "\n",
    "Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "\n",
    "Bagging attempts to reduce the chance overfitting complex models.\n",
    "\n",
    "It trains a large number of \"strong\" learners in parallel. A strong learner is a model that's relatively unconstrained. Bagging then combines all the strong learners together in order to \"smooth out\" their predictions. Boosting attempts to improve the predictive flexibility of simple models.\n",
    "\n",
    "It trains a large number of \"weak\" learners in sequence. A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree). Each one in the sequence focuses on learning from the mistakes of the one before it. Boosting then combines all the weak learners into a single strong learner. While bagging and boosting are both ensemble methods, they approach the problem from opposite directions.\n",
    "\n",
    "Bagging uses complex base models and tries to \"smooth out\" their predictions, while boosting uses simple base models and tries to \"boost\" their aggregate complexity.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
